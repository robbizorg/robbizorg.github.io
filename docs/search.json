[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Robin Netzorg",
    "section": "",
    "text": "A girl close to noise.\nI’m Robin Netzorg (she/her), a fifth-year PhD student in UC Berkeley’s EECS Department, advised by Prof. Gopala Anumanchipalli and Prof. Bin Yu. Research-wise, I’m currently interested in flexible speech synthesis and voice modification, with a particular interest in applying these techniques to build tools for speech therapy and Gender-Affirming Voice Training (GAVT). Previously, I was interested in interpretable machine learning and causal inference, but, well, sometimes you just have to make a drastic life change in the fourth year of your PhD.\nOutside of research, I’m your semi-stereotypical Bay Area trans girl. I’m into making “music” with my friends, playing video games, reading books, and jotting down thoughts in my journal. Check out my research below or click here to see whatever it is I’ve decided to post."
  },
  {
    "objectID": "index.html#selected-research",
    "href": "index.html#selected-research",
    "title": "Robin Netzorg",
    "section": "Selected Research",
    "text": "Selected Research\n\n\n\n\n  \n\n\n\n\nPerMod: Perceptually Grounded Voice Modification with Latent Diffusion Models\n\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2023\n\n\nRobin Netzorg, Ajil Jalal, Luna McNulty, Gopala Anumanchipalli\n\n\n\n\n\n\n  \n\n\n\n\nTowards an Interpretable Representation of Speaker Identity via Perceptual Voice Qualities\n\n\n\n\n\n\n\n\n\n\n\n\nOct 4, 2023\n\n\nRobin Netzorg, Bohan Yu, Andrea Guzman, Peter Wu, Luna McNulty, Gopala Anumanchipalli\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "papers/permod/index.html#abstract",
    "href": "papers/permod/index.html#abstract",
    "title": "PerMod: Perceptually Grounded Voice Modification with Latent Diffusion Models",
    "section": "Abstract",
    "text": "Abstract\nPerceptual modification of voice is an elusive goal. While non-experts can modify an image or sentence perceptually with available tools, it is not clear how to similarly modify speech along perceptual axes. Voice conversion does make it possible to convert one voice to another, but these modifications are handled by black box models, and the specifics of what perceptual qualities to modify how to modify them are unclear. Towards allowing greater perceptual control over voice, we introduce PerMod, a conditional latent diffusion model that takes in an input voice and a perceptual qualities vector, and produces a voice with the matching perceptual qualities. Unlike prior work, PerMod generates a new voice corresponding to perceptual modifications. Evaluating perceptual quality vectors with RMSE from both human and predicted labels, we demonstrate that PerMod produces voices with the desired perceptual qualities for typical voices, but performs poorly on atypical voices."
  },
  {
    "objectID": "posts/hello_again/index.html",
    "href": "posts/hello_again/index.html",
    "title": "Finally Made a Website. Aka, Welcome to My Life",
    "section": "",
    "text": "A girl who has her shit life together\nAfter years and years, and years, and years and years and years, I made a website. The first time I ever talked about wanting to make a website was approximately on December 28, 2017. So that’s, what, since my junior year of college I’ve been thinking and failing to make a website? Let’s code out how long it’s been, cause why not:\nCode\nfrom datetime import datetime\n\n# Start and end dates\nstart_date = datetime(2017, 12, 28)\nend_date = datetime(2023, 11, 4)\n\n# Get that difference\ndifference = end_date - start_date\n\n# Conversionssss\nyears = difference.days // 365\nmonths = (difference.days % 365) // 30\ndays = (difference.days % 365) % 30\n\nprint(f\"Beep boop beep: It took you {years} years, {months} months, and {days} days to make me! Beep boop beep.\")\n\n\nBeep boop beep: It took you 5 years, 10 months, and 12 days to make me! Beep boop beep.\nJesus H. Christ. It took me almost six years to actually get around to building this thing! If you asked college junior me how I’d feel knowing that it would take me six years to put together a simple website, I’d probably have responded with initial disbelief at future girl-me, horrendous confusion as to why the first thing she’d bring up is a stupid website, and then a impending sense of dread at my own inability to be able to stick to the things that I “needed” to do.\nMight be obvious by the fact that it took me six years to get around to it, but, as it turns out, a personal website isn’t necessary to your career or personal development at all. Even as an unsuccessful PhD student, a personal website didn’t really matter much. Arguably, it’s probably a better thing not to have one. The self-importance, the stupifying effect of marketing, people learning what I actually think. Shudder. Not good things.\nBut here I am six years later, having made a personal website. Why? I could pontificate for paragraphs about the need to market oneself in today’s economy in order to maximize the impact and visibility of one’s work, but I’ll spare you the details. Just know, a big part of this is probably because I feel like I actually have work to share now, that it’s important that people see this work, and that’s why I’m making this.\nThere’s this article I’ve been wanting to write for years, “To Share or Not to Share”. Well, I used to want to write it. back when I still thought I was neurotypical enough to be able to handle more than one task at a time, to be able to take the time to craft out a well-reasoned and researched argument while being a “full-time” grad student. So easy to beat yourself up over that stuff.\nAnyways! The gist of the article is simple, and getting it out in some form is better than not it all, so here’s the grand question I felt like I had to share with the world around me: In this hyperactive, distracted, terminally online age, if I don’t share my life, do I exist? I know I got so much stuff going on that I sometimes forget about people until I see their content pop up somewhere. How true is that for others? Do people only think of me when a picture I post pops up on their feed? If I spend all my time alone, not sharing and posting my life and thoughts, am I even real?\n(A bit selfish, don’t you think?)\nIn a former life, I studied popularity on Twitch, and what we found in the course of our non-causal study was clear. Popularity is a combination of hard work and blind luck. Research! Betting on blind luck seems like a losing strategy, so may as well lean into whatever this hard work thing is. We couldn’t really measure content quality, but it was pretty clear that the people who grew in popularity the most were very active content creators.\nIt makes sense, you’re not going to get people to pay attention to you unless you’re something that can be perceived in the first place. And on Twitch, the way to do that is stream. On pretty much any social media website, the way to do that is post. In real life, at least here in the US, where we probably don’t live close to the people we care about, where we probably won’t run into friends in our day-to-day routines unless it’s planned, where the addictive allure and perceived safety of anonymity online compels us to lie indoors in catatonic depression, how do you be perceivable?\nIt’s simple. You share.\nIt really doesn’t matter what you share, or how you share it. As long as a little notification pops up that has your name attached to it, that person had to think of you for that brief moment. For that moment, you existed outside of that little head of yours. If you do it enough, maybe they start developing a need for that notification, craving it every time they open their phone. Maybe they dread it. Maybe there’s an app keeping count, reminding that that if they don’t think of you, they’re going to lose all some kind of streak. Maybe they can’t wait to think of you, so they find a community of people who just think about you. Then you could look yourself up on Google and confirm what you hoped to be true all along:\nUgh, the Internet sucks so much.\nThis is obviously just a tad bit insane, but there’s definitely some kernel of truth to it. The nagging thought over the yeras of making a website probably came from some version of this need to exist outside of my own head. All this before I needed people to affirm my identity as a woman. Do you know how much I want people to perceive me for who I know myself to be? That little existential validation when a friend calls me pretty–how could I resist sharing a pic or two?\nAt the same time, is your online-self actually you? Are you representing yourself how you want to be represented? Are people perceiving you the way you want to be perceived? I feel like it’s so easy to lose yourself by obsessing over these questions, not to mention the real chance of harm from that people can inflict on one another…\nTo share or not to share, that is the question."
  },
  {
    "objectID": "posts/hello_again/index.html#the-sunday-sophist",
    "href": "posts/hello_again/index.html#the-sunday-sophist",
    "title": "Finally Made a Website. Aka, Welcome to My Life",
    "section": "The Sunday Sophist",
    "text": "The Sunday Sophist\nYears ago, back when I thought I was neurotypical enough to handle more than one task at a time, I dreamt of releasing semi-regular essays on my random musings. Turns out, I didn’t have the time, energy, or dedication to do it, but maybe I can do a not semi-regular posting of trivial thoughts?\nWhen I was back in college, I somehow ended up in the position of being a teaching assistant for a graduate-level class. Masters students were going to some 19 year old kid for advice on their AI home. Imposter syndrome was real (is it ever not?), and I thought of myself as a bit of a sophist–literally a teacher these people were paying who had less life and educational experience than them.\nAcademics, with our air of legitimacy, citation counts, and wasted years, have a tendency to overinflate the importance of the thoughts we have, or their validity. We build off of prior work, point to the textbook, and use sound, reasonable logic to make our claims. We are to be believed, we say. You can trust us. We, after all, are the experts.\nI was hoping that at some point I’d feel less like an imposter, some kind of sophist. Eventually, with enough experience and affirmation, I’d develop my own sense of security in my knowledge. If I read enough, write enough, and think enough, I’d finally be confident in what I believe and my ability to teach it. I’d be a real expert. Eventually, I’d finally have something worth saying!\nThrough the years, and I mean years, that feeling of security has never come. In times when my career was going well (which has it ever gone well?), to times when my career was not going well, I’ve never felt secure in what I’m doing or what I know. So many people are wrong, regardless of their sound logic and mountains of references, how would I know if I’m ever right?\nI don’t, and that’s okay. To be honest, I don’t think I ever want to know if I’m right or not. That sort of certainty sounds scary. Besides, the rush of fear when someone points out a flaw in my reasoning is way more exciting. That uncertainty in it of itself is reassuring. I trust myself to change my opinion when I’m wrong, to always doubt myself just enough to not take myself too seriously, to be kind of happy with being a sophist. Who’s really real, anyways?\n\nWelcome to my website. Based on the available readership statistics, the vast majority of you never make it down this far anyways. But if you did, thank you, I guess? I’ve been dreading making a website, but, at this point in my career, I feel like I have to. Maybe I’ll expand on that at some point. Maybe I won’t. Who knows? I suffer from the unfortunate affliction of wanting to write for the sake of writing, even if I have nothing to say. I’ll probably ramble more unsubstantiated claims about a variety of topics over the time to come. And if you’re wondering why I would ever share these thoughts in my head, let met tell you this: I’m sharing them because I want to. Where that want comes from, I don’t know. Are they worth sharing, and am I someone who’s worth listening to? Probably not.\nWho’s worth listening to anyways?"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Tutorials, Demos, and Random, Random Musings",
    "section": "",
    "text": "Finally Made a Website. Aka, Welcome to My Life\n\n\n\n\n\n\n\nblog\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nRobbie\n\n\n\n\n\n\n  \n\n\n\n\nVoice Research #1: Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn\n\n\n\n\n\n\n\nperceptual qualities\n\n\nvoice research\n\n\n\n\n\n\n\n\n\n\n\nNov 4, 2023\n\n\nRobin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "papers/pq_rep/index.html#abstract",
    "href": "papers/pq_rep/index.html#abstract",
    "title": "Towards an Interpretable Representation of Speaker Identity via Perceptual Voice Qualities",
    "section": "Abstract",
    "text": "Abstract\nUnlike other data modalities such as text and vision, speech does not lend itself to easy interpretation. While lay people can understand how to describe an image or sentence via perception, non-expert descriptions of speech often end at high-level demographic information, such as gender or age. In this paper, we propose a possible interpretable representation of speaker identity based on perceptual voice qualities (PQs). By adding gendered PQs to the pathology-focused Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) protocol, our PQ-based approach provides a perceptual latent space of the character of adult voices that is an intermediary of abstraction between high-level demographics and low-level acoustic, physical, or learned representations. Contrary to prior belief, we demonstrate that these PQs are hearable by ensembles of non-experts, and further demonstrate that the information encoded in a PQ-based representation is predictable by various speech representations."
  },
  {
    "objectID": "posts/explore_pvqd/index.html",
    "href": "posts/explore_pvqd/index.html",
    "title": "Voice Research #1: Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "",
    "text": "Another day, another tutorial\nWelcome to a blog post about exploring the Perceptual Voice Qualities Database (PVQD). This is based off of a ML + Speech workshop I held in the Berkeley Speech Group in Fall ’23. That workshop and the workbooks are available online if you want to work with the code yourself! If you’re interested in Deep Learning, there’s also some cool stuff in there about working with HuBERT representations of speech data too! The workshop doesn’t assume any knowledge of Pandas, Machine Learning, or Deep Learning, with the focus is more on getting your hands dirty with the tools vs. understanding 100% of what you’re doing.\nThat said, if you want to learn more about the tools, the repo contains a list of resources for learning more.\nAnyways, if I’m linking to that repo, you might be wondering, why write a blog post about it? First, who doesn’t love a bit of redundancy? Second, not everyone is going to want to actually work with the code themselves. I want to communicate and share the stuff I’ve been working on, and generally share the thought processes I have when trying to work with Speech data. This is to get feedback on my work from folks who might not know a lot about data science and machine learning. This is also to serve as a type of onboarding and explanation tool for those interested in conducting voice research with publicly available datasets. Finally, I really, really need just say what I do in plain english!\nAnd the plain English explanation is simple. Or, well, at least short. I use data science and machine learning to study voice and voice modification. I’m particularly interested in possible applications to transgender and gender-diverse voice training."
  },
  {
    "objectID": "posts/explore_pvqd/index.html#what-is-the-perceptual-voice-qualities-database",
    "href": "posts/explore_pvqd/index.html#what-is-the-perceptual-voice-qualities-database",
    "title": "Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "What is the Perceptual Voice Qualities Database?",
    "text": "What is the Perceptual Voice Qualities Database?"
  },
  {
    "objectID": "posts/explore_pvqd/index.html#exploration",
    "href": "posts/explore_pvqd/index.html#exploration",
    "title": "Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "Exploration!",
    "text": "Exploration!"
  },
  {
    "objectID": "posts/explore_pvqd/index.html#what-is-pandas-and-data-science",
    "href": "posts/explore_pvqd/index.html#what-is-pandas-and-data-science",
    "title": "Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "What is Pandas and Data Science?",
    "text": "What is Pandas and Data Science?"
  },
  {
    "objectID": "posts/explore_pvqd/index.html#what-is-perceptual-voice-quality",
    "href": "posts/explore_pvqd/index.html#what-is-perceptual-voice-quality",
    "title": "Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "What is Perceptual Voice Quality?",
    "text": "What is Perceptual Voice Quality?"
  },
  {
    "objectID": "posts/explore_pvqd/index.html#what-is-perceptual-voice-quality-and-the-pvqd",
    "href": "posts/explore_pvqd/index.html#what-is-perceptual-voice-quality-and-the-pvqd",
    "title": "Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "What is Perceptual Voice Quality and the PVQD?",
    "text": "What is Perceptual Voice Quality and the PVQD?"
  },
  {
    "objectID": "posts/explore_pvqd/index.html#what-sort-of-research-do-you-do",
    "href": "posts/explore_pvqd/index.html#what-sort-of-research-do-you-do",
    "title": "Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "What Sort of Research Do You Do?",
    "text": "What Sort of Research Do You Do?\nThat’s a question for another blog post."
  },
  {
    "objectID": "posts/explore_pvqd/index.html#what-are-perceptual-voice-qualities-and-the-pvqd",
    "href": "posts/explore_pvqd/index.html#what-are-perceptual-voice-qualities-and-the-pvqd",
    "title": "Voice Research #1: Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "What are Perceptual Voice Qualities and the PVQD?",
    "text": "What are Perceptual Voice Qualities and the PVQD?\nThought of as the acoustic “coloring” of an individual’s voice, perceptual qualities are one way that clinicians and voice experts conceptualize the subjective perception of a voice. Here, we provide examples of the CAPE-V perceptual qualities we use in our work. For a description of the gendered perceptual qualities of resonance and weight, we recommend taking a look over voice examples from the Voice Resource Repository by SumianVoice.\nTalking about perceptual qualities in the abstract is good and all, but if you want to actually understand anything related to voice, you’re going to need to listen to them. While the CAPE-V protocol is a helpful rubric for working perceptual qualities related to vocal health, most of the voice data and assessments are privately held by voice clinics to protect the privacy of patients. Fortunately, there are a few publicly available datasets like the Perceptual Voice Qualities Database (PVQD) that contain voice samples and expert-provided ratings. In this tutorial, we’ll explore the PVQD and the CAPE-V perceptual qualities!\n\nPerceptual Qualities Examples\nHere’s a list of examples of the perceptual qualities from the PVQD and their definitions per the CAPE-V Protocol.\nStrain: Perception of vocal effort (hyperfunction).\n\n\nCode\nimport IPython\nIPython.display.Audio(\"../../data/pvqd/examples/pt0015_strain.wav\")\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nLoudness: Deviation in perceived loudness typical for that speaker’s age and gender.\n\n\nCode\nimport IPython\nIPython.display.Audio(\"../../data/pvqd/examples/pt008_loud.wav\")\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nRoughness: Perceived irregularity in the voicing source.\n\n\nCode\nimport IPython\nIPython.display.Audio(\"../../data/pvqd/examples/pt0014_rough.wav\")\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nBreathiness: Audible air escape in the voice.\n\n\nCode\nimport IPython\nIPython.display.Audio(\"../../data/pvqd/examples/pt118_breath.wav\")\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nPitch: Deviation in pitch values typical for that speaker’s age and gender.\n\n\nCode\nimport IPython\nIPython.display.Audio(\"../../data/pvqd/examples/pt019_pitch.wav\")\n\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "posts/explore_pvqd/index.html#what-are-pandas-and-seaborn",
    "href": "posts/explore_pvqd/index.html#what-are-pandas-and-seaborn",
    "title": "Voice Research #1: Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "What are Pandas and Seaborn?",
    "text": "What are Pandas and Seaborn?\nIf you’re interested in doing anything with data, you’re going to need a way to look at it and mess wtih it. In the world of Python, one way to do that is with Pandas and Seaborn. Pandas for manipulating the data and getting it in the format you want, and Seaborn for quickly visualizing it. These are great tools for Exploratory Data Analysis (EDA), which, to butcher it with my own paraphrased phrasing, is the process of getting to know your data and understanding what possible patterns might be there. EDA is its own exciting research field, and doing it robustly requires a lot of thought about the sort of questions you’re asking. For the purposes of this tutorial, knowing that Pandas and Seaborn are two tools for performing exploratory data analysis is good enough background to get started looking at the PVQD."
  },
  {
    "objectID": "posts/explore_pvqd/index.html#lets-do-some-viz",
    "href": "posts/explore_pvqd/index.html#lets-do-some-viz",
    "title": "Voice Research #1: Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "Let’s Do Some Viz",
    "text": "Let’s Do Some Viz\nIf you’re interested in studying how voice varies across demographics and vocal health, the PVQD is a great starting point. Not only does the dataset contain ~2hrs of audio from vastly different speakers, the data also comes with demographic information about the speakers as well. Let’s visualize that now.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\ndemo_df = pd.read_csv(\"../../data/pvqd/pvqd_demographics.csv\")\ndemo_df.head(5)\n\n\n\n\n\n\n\n\n\nParticipant ID\nGender\nAge\nDiagnosis\n\n\n\n\n0\nBL01\nM\n46.0\nNaN\n\n\n1\nBL02\nF\n76.0\nNaN\n\n\n2\nBL03\nF\n88.0\nNaN\n\n\n3\nBL04\nF\n42.0\nNaN\n\n\n4\nBL05\nF\n75.0\nNaN\n\n\n\n\n\n\n\nWe see that the PVQD comes with three pieces of demographic information: the participants’ Gender, Age, Diagnosis. Frustratingly as someone interested in studying transgender and gender-diverse voices, there aren’t that many datasets out there that contain more inclusive labels of gender. Gender, in the vast majority of datasets, is listed as a binary “F” or “M”. In addition to binary labels of gender, from my personal experience, most of publicly available speech datasets do not contain non-cisgender speakers. For the construction of machine learning models that can provide feedback on gender affirming voice training, this is a huge limitation. Machine learning methods often fail to generalize to unseen data, especially when that data is “out of distribution”.\nAlthough we won’t be able to get a complete map of what voice is capable of, we can start to get a rough one. The nice thing about the PVQD is that it does provide us with voices that do exhibit certain qualities we might want to avoid during voice training, such as breathiness and strain. Let’s load in that data while we’re talking about it:\n\n\nCode\npq_train = pd.read_csv(\"../../data/pvqd/train_test_split/y_train.csv\", index_col=0)\npq_train.head(5)\n\n\n\n\n\n\n\n\n\nFile\nBreathiness\nLoudness\nPitch\nRoughness\nStrain\n\n\n\n\n0\nLA7005\n7.000000\n7.333333\n7.000000\n8.500000\n8.000000\n\n\n1\nPT116\n31.833333\n12.333333\n6.000000\n29.500000\n7.333333\n\n\n2\nPT034\n43.166667\n26.666667\n61.166667\n34.000000\n30.333333\n\n\n3\nSJ6006\n4.375000\n5.125000\n0.250000\n3.750000\n6.875000\n\n\n4\nPT005\n5.500000\n7.000000\n12.000000\n3.833333\n4.500000\n\n\n\n\n\n\n\nAs we said before, the CAPE-V measures perceptual qualities on a 1-100 scale. Since these values are the average of six different raitngs, we get all those decimal places."
  },
  {
    "objectID": "posts/explore_pvqd/index.html#why-use-data-science-and-machine-learning-to-study-voice",
    "href": "posts/explore_pvqd/index.html#why-use-data-science-and-machine-learning-to-study-voice",
    "title": "Voice Research #1: Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "Why Use Data Science and Machine Learning to Study Voice?",
    "text": "Why Use Data Science and Machine Learning to Study Voice?\nThis question deserves an entire post of its own, not to mention a long discussion on the state of research in voice and voice training more generally. Still, any good tutorial requires a motivation, and the oversimplified version is this: When it comes to modifying a voice, both artificially via speech processing and physically via voice training, the current research is lacking. In speech processing, high-level modification of a particular voice is feasible, such as automatically turning a specific masculine voice into a specific feminine voice, but trying to perform lower-level modification of qualities like breathiness, strain, or resonance is either unclear or requires specialized expert knowledge. In the world of voice and voice training, research into Gender-Affirming Voice Care (GAVC) is only really starting to gain traction in Speech Language Pathology, and many of the pedagogies people pursue from online trans voice communities have not been validated experimentally or quantitatively in a formal setting. From an institutional, formal perspective, the effectiveness of behavioral voice modification, and particular approaches and pedagogies, is largely unknown.\nThere’s perspectives waiting to be studied, methods to be discovered, and misconceptions to be debunked. The collection and analysis of data, combined with the modeling potential of machine learning, can help in those pursuits. How? That’s a topic for a post of its own. For the time being, let’s go ahead and study some perceptual voice qualities."
  },
  {
    "objectID": "papers/permod/index.html",
    "href": "papers/permod/index.html",
    "title": "PerMod: Perceptually Grounded Voice Modification with Latent Diffusion Models",
    "section": "",
    "text": "Demo"
  },
  {
    "objectID": "posts/explore_pvqd/index.html#preprocessing-the-pvqd",
    "href": "posts/explore_pvqd/index.html#preprocessing-the-pvqd",
    "title": "Voice Research #1: Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "Preprocessing the PVQD",
    "text": "Preprocessing the PVQD\nIf you’re interested in studying how voice varies across demographics and vocal health, the PVQD is a great starting point. Not only does the dataset contain ~2hrs of audio from vastly different speakers, the data also comes with demographic information about the speakers as well. Let’s visualize that now.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nplt.rcdefaults() # Default\n\ndemo_df = pd.read_csv(\"../../data/pvqd/pvqd_demographics.csv\")\ndemo_df.head(5)\n\n\n\n\n\n\n\n\n\nParticipant ID\nGender\nAge\nDiagnosis\n\n\n\n\n0\nBL01\nM\n46\nNaN\n\n\n1\nBL02\nF\n76\nNaN\n\n\n2\nBL03\nF\n88\nNaN\n\n\n3\nBL04\nF\n42\nNaN\n\n\n4\nBL05\nF\n75\nNaN\n\n\n\n\n\n\n\nWe see that the PVQD comes with three pieces of demographic information: the participants’ Gender, Age, Diagnosis. Frustratingly as someone interested in studying transgender and gender-diverse voices, there aren’t that many datasets out there that contain more inclusive labels of gender. Gender, in the vast majority of datasets, is listed as a binary “F” or “M”. In addition to binary labels of gender, from my personal experience, most of publicly available speech datasets do not contain non-cisgender speakers. For the construction of machine learning models that can provide feedback on gender affirming voice training, this is a huge limitation. Machine learning methods often fail to generalize to unseen data, especially when that data is “out of distribution”.\nAlthough we won’t be able to get a complete map of what voice is capable of, we can start to get a rough one. The nice thing about the PVQD is that it does provide us with voices that do exhibit certain qualities we might want to avoid during voice training, such as breathiness and strain. Let’s load in that data while we’re talking about it:\n\n\nCode\npq_train = pd.read_csv(\"../../data/pvqd/train_test_split/y_train.csv\", index_col=0)\npq_train.head(5)\n\n\n\n\n\n\n\n\n\nFile\nBreathiness\nLoudness\nPitch\nRoughness\nStrain\n\n\n\n\n0\nLA7005\n7.000000\n7.333333\n7.000000\n8.500000\n8.000000\n\n\n1\nPT116\n31.833333\n12.333333\n6.000000\n29.500000\n7.333333\n\n\n2\nPT034\n43.166667\n26.666667\n61.166667\n34.000000\n30.333333\n\n\n3\nSJ6006\n4.375000\n5.125000\n0.250000\n3.750000\n6.875000\n\n\n4\nPT005\n5.500000\n7.000000\n12.000000\n3.833333\n4.500000\n\n\n\n\n\n\n\nAs we said before, the CAPE-V measures perceptual qualities on a 1-100 scale. Since these values are the average of six different raitngs, we get all those decimal places. It’s nice to look at the data like that, to make sure it’s there, but we’re still a bit away from actually learning anything about the data. I’m brushing this part under the rug (see the workshop if you’re curious!), but I’m going to clean up the data and combine the two dataframes so we can start drawing some connections.\n\n\nCode\n### Fix up the demo_df first\n\n## Fix Gender\ncor_gender = {\n    \"f\": \"F\",\n    \"m\": \"M\",\n    \"female\": \"F\",\n    \"male\": \"M\"\n}\n\ndemo_df[\"Gender\"] = demo_df[\"Gender\"].str.lower().map(cor_gender)\n\n## Make Binary Value for Diagnosis\n# Create a function to apply to the column\ndef process_diagnosis(diagnosis):\n    if pd.isnull(diagnosis):\n        return np.nan\n    if diagnosis == \"N\":\n        return \"N\"\n    else:\n        return \"Y\"\n\ndemo_df[\"Diagnosis \"] = demo_df[\"Diagnosis \"].map(process_diagnosis)\n\n## We can't visualize nan data, so let's not consider those rows\n# ~ is a way to invert a numpy array/pandas series of boolean values. A handy trick\ndiagnosis_df = demo_df[~pd.isnull(demo_df[\"Diagnosis \"])]\n\n# Rename column and remote white space\ndiagnosis_df = diagnosis_df.rename(columns={\"Participant ID \": \"File\"})\ndiagnosis_df[\"File\"] = diagnosis_df[\"File\"].str.upper().str.strip()\n\n## Combine the Data\n\n# Merge the two dataframes using a left join\nmatched_df = pq_train.merge(diagnosis_df, how=\"left\", on=\"File\")\n\nmatched_df = matched_df[~pd.isnull(matched_df[\"Gender\"])]\n\nprint(\"Number of NaN Values: %s\" % len(matched_df[pd.isnull(matched_df[\"Gender\"])]))\n\nmatched_df.head()\n\n\nNumber of NaN Values: 0\n\n\n\n\n\n\n\n\n\nFile\nBreathiness\nLoudness\nPitch\nRoughness\nStrain\nGender\nAge\nDiagnosis\n\n\n\n\n0\nLA7005\n7.000000\n7.333333\n7.000000\n8.500000\n8.000000\nF\n52.0\nY\n\n\n1\nPT116\n31.833333\n12.333333\n6.000000\n29.500000\n7.333333\nF\n81.0\nY\n\n\n2\nPT034\n43.166667\n26.666667\n61.166667\n34.000000\n30.333333\nF\n80.0\nY\n\n\n3\nSJ6006\n4.375000\n5.125000\n0.250000\n3.750000\n6.875000\nF\n17.0\nN\n\n\n4\nPT005\n5.500000\n7.000000\n12.000000\n3.833333\n4.500000\nF\n58.0\nY\n\n\n\n\n\n\n\nCool! We have our combined dataset that has all the information in one place."
  },
  {
    "objectID": "posts/explore_pvqd/index.html#visualizing-the-pvqd",
    "href": "posts/explore_pvqd/index.html#visualizing-the-pvqd",
    "title": "Voice Research #1: Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "Visualizing the PVQD",
    "text": "Visualizing the PVQD\nLet’s start doing our explorations! The first thing we should figure out is what the age and gender makeup of the dataset looks like.\n\n\nCode\ngender_breakdown = diagnosis_df[[\"Gender\", \"Age\"]].groupby(\"Gender\").agg([\"mean\", \"count\"]).reset_index()\n\nsns.histplot(data=diagnosis_df, x=\"Age\", hue=\"Gender\")\nplt.show()\n\ngender_breakdown\n\n\n\n\n\n\n\n\n\n\n\n\nGender\nAge\n\n\n\n\nmean\ncount\n\n\n\n\n0\nF\n43.861878\n181\n\n\n1\nM\n50.020833\n96\n\n\n\n\n\n\n\nSeems like there’s about twice as many women in the dataset than men, with many of the women being younger on average.\nWhat about the age and diagnosis breakdown?\n\n\nCode\nsns.histplot(data=diagnosis_df, x=\"Age\", hue=\"Diagnosis \")\nplt.show()\n\n\n\n\n\nWhile there are quite a few young people with diagnosis, we see that healthy older individuals are underrepresented in the dataset, and the majority of those with diagnoses skew older.\nFrom this point on, it’s important to note that I’m only dealing with the training set to avoid biasing the results we find. It is okay to compute statistics about the demographics over the entire dataset, but, since we are hoping to build models that learn the perceptual qualities, performing data exploration with them in the data could possibly bias our findings. To make sure we have a proper test set to evaluate our findings on, we’ll conduct our exploration on the training data.\nLet’s go ahead and visualize the relationship between vocal health and the CAPE-V ratings as reported in the PVQD. First, we’ll compute some aggregate statistics between speakers with diagnoses and those without.\n\n\nCode\n# Make sure that the diagnosis_df's File column matches that of pq_train\ndiagnosis_df[\"File\"] = diagnosis_df[\"File\"].str.upper().str.strip()\n\n\n# Merge the two dataframes using a left join\nmatched_df = pq_train.merge(diagnosis_df, how=\"left\", on=\"File\")\n\n# Filter out the NaN Examples and Run a Quick Check\nmatched_df = matched_df[~pd.isnull(matched_df[\"Gender\"])]\n\nassert len(matched_df[pd.isnull(matched_df[\"Gender\"])]) == 0\n\n# Here's a df of the PQs and the diagnosis value\npq_and_diagnosis_df = matched_df[[\"Diagnosis \", \"Breathiness\", \"Loudness\", \"Pitch\", \"Roughness\", \"Strain\"]]\n\n# TODO: Calculate the Average, Median, and Max PQ per diagnosis\npq_and_diagnosis_df.groupby(\"Diagnosis \").agg([\"mean\", \"median\", \"max\"]).round(2)\n\n\n\n\n\n\n\n\n\nBreathiness\nLoudness\nPitch\nRoughness\nStrain\n\n\n\nmean\nmedian\nmax\nmean\nmedian\nmax\nmean\nmedian\nmax\nmean\nmedian\nmax\nmean\nmedian\nmax\n\n\nDiagnosis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nN\n8.81\n5.50\n38.0\n4.24\n2.94\n21.83\n4.69\n2.33\n20.83\n11.76\n10.17\n44.62\n9.88\n8.21\n32.50\n\n\nY\n24.06\n16.17\n99.5\n24.49\n14.67\n99.17\n22.02\n12.17\n99.17\n25.78\n21.50\n84.83\n26.20\n18.17\n96.83\n\n\n\n\n\n\n\nThis isn’t the prettiest table, but it gets the point across. We see the sort of trends we’d expect. Comparing means and medians across perceptual qualities, we note that voices with diagnoses have higher ratings than those without, with the maximum values for diagnosed speakers across all PQs except roughness being close to 100. If you’re familiar with statistics, you might be wondering if these differences are statistically significant. I leave it as an exercise to the reader to test this question in the associated workbooks! For this exploration, I’m happy enough to note that the difference we’d expect to see is indeed there. The max for non-diagnosed voices is lower, but can vary greatly from PQ to PQ. Breathiness, for example, has a maximum value of 38.0 while loudness has a maximum value of 21.83.\nLet’s visualize the distributions to understand if these are flukes or a larger part of the distribution. Orange represents speakers without a diagnosis.\n\n\nCode\n# Same df as before, but going to include the File to make it clearer\npq_and_diagnosis_df = matched_df[[\"File\", \"Age\", \"Diagnosis \", \"Breathiness\", \"Loudness\", \"Pitch\", \"Roughness\", \"Strain\"]]\n\n# Make a Long DataFrame\nlong_df = pd.melt(pq_and_diagnosis_df, id_vars = [\"File\", \"Age\", \"Diagnosis \"], value_vars=[\"Breathiness\", \"Loudness\", \"Pitch\", \"Roughness\", \"Strain\"], var_name = \"PQ\", value_name = \"Value\")\n\n\n# Make a FacetGrid to plot a lot of things all at once\ng = sns.FacetGrid(data=long_df, col = \"PQ\", hue = \"Diagnosis \")\ng.map(sns.histplot, \"Value\")\n\n\n\n\n\nCool! It seems like it kind of is a fluke, with the highest value being around 25-30 for many of the non-diagnosed voices. It would be good to compute different quartiles or a CDF right about now to more rigorously confirm this, but also going to leave this to the curious reader :D\nWe still haven’t looked at age and these perceptual qualities yet. Let’s go ahead and combine the above plots with an age dimension. Orange again represents voices without a diagnosis.\n\n\nCode\n# Make a FacetGrid to plot a lot of things all at once\ng = sns.FacetGrid(data=long_df, col = \"PQ\", hue = \"Diagnosis \")\ng.map(sns.scatterplot, \"Age\", \"Value\")\n\n\n\n\n\nIt’s interesting to note that across PQs there tends to be a slight correlation between age and the perceptual quality, with a few of the PQs, like strain, not having any examples of folks who are younger than 30 with highly strained voices. Some PQs do have examples for younger individuals, but even then it’s not many."
  },
  {
    "objectID": "posts/explore_pvqd/index.html#footnotes",
    "href": "posts/explore_pvqd/index.html#footnotes",
    "title": "Voice Research #1: Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe ‘coloring’ of a voice that that makes each voice unique. See the next section!↩︎"
  },
  {
    "objectID": "posts/explore_pvqd/index.html#what-did-we-learn",
    "href": "posts/explore_pvqd/index.html#what-did-we-learn",
    "title": "Voice Research #1: Exploring the Perceptual Voice Qualities Database with Pandas and Seaborn",
    "section": "What did we learn?",
    "text": "What did we learn?\nThere’s one main takeaway we walk away with by performing this data exploration: we learn about the data distributions and correlations present in the PVQD data (under our particular set of processing choices). This can tell us about the limits of various modeling approaches we can expect, or failures of generation. For example, if we were to train a model that sought to return a 1-100 value for strain from an individual’s voice, if that individual is young and has a strained voice, our model might not be able to generalize to this new datapoint–possibly returning that the voice is less strained than it actually is. In this case, the model would have too strongly associated strain with age-specific vocal features, instead of learning what strain is across age groups. This visualization and exploration helps point out possible failures of modeling we could expect.\nAnother takeaway from this exploration is how SLPs label audio data ccording to the CAPE-V protocol. If we wished to incorporate different types of data, or perhaps even learn how to provide CAPE-V ratings ourselves, it’s incredibly important to know the difference between voices with high and low values of a particular perceptual quality. Given that this dataset is primarily concerned with patients with voice disorders, knowing the values that healthy or “typical” voices might have lets us know what PQ values healthy voices in other datasets should have. Generalization is a huge issue for machine learning and deep learning models. Having knowledge about how experts use the CAPE-V protocol across voices facilitates the future collection of more data to mitigate the generalization issue.\nIn the next post, we’re going to try to and actually model some of these perceptual qualities. Let’s see if we can do as the experts do!"
  }
]